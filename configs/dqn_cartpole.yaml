model_kwargs:
  learning_rate: 0.0005
  gamma: 0.99
  buffer_size: 100000
  batch_size: 128
  learning_starts: 10000
  target_update_interval: 5000
  exploration_fraction: 0.15
  exploration_final_eps: 0.02
  train_freq: 4
  gradient_steps: 1
  policy_kwargs:
    net_arch: [256, 256]
    activation_fn: ReLU
